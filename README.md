# MINILLM ‚Äì Simple Chatbot Based on GPT

**MINILLM** is a compact, self-developed LLM (Large Language Model). It aims to provide a better understanding of AI models for both myself and others. It offers a simple way to train, test, and customize a language model. This project is intended for both enthusiasts and developers who want to experiment on their own. Anyone can copy this project to their computer.

## 1. Overview of MINILLM

### Comparison of Popular LLMs

| Model       | Developer      | Key Features                                      |
|------------|--------------|--------------------------------------------------|
| **ChatGPT**  | OpenAI        | Versatile chatbot AI, trained on vast datasets |
| **Grok**     | xAI (Elon Musk) | Humorous chatbot with sharp responses          |
| **DeepSeek** | Open-Source   | Efficient and free LLM from China              |

MINILLM is a **minimalist version of these models**, running locally and fully customizable.

## 2. The Complete Process ‚Äì From Model to Chatbot

The development of a language model like MINILLM consists of several steps:

1. **Load a pre-trained model** ‚Äì We use an already trained model (GPT-2) from Hugging Face instead of training one from scratch.
2. **Provide datasets for training** ‚Äì The model is further trained with a dataset (e.g., OpenWebText) to adapt it to desired inputs.
3. **Tokenization & Processing** ‚Äì Text is converted into tokens that the model can understand. Short texts are padded, and long texts are truncated if necessary.
4. **Training the Model** ‚Äì The model runs through the training data multiple times (epochs) and learns to predict text by calculating the next token based on previous tokens. Weights in the neural network are adjusted, enabling the model to recognize patterns and improve.
5. **Fine-tuning and Testing** ‚Äì After training, the model is tested and can be used for various applications.

### What is OpenWebText as a Dataset?

OpenWebText is a freely available dataset consisting of high-quality web texts. It was created as an alternative to OpenAI‚Äôs GPT-2 training data and contains texts from reputable sources filtered based on specific quality criteria.

### Why Must All Inputs Have the Same Length? (Padding & Truncation)

Neural networks operate more efficiently when all inputs have the same length. To ensure the model trains in fixed blocks:
- **Short inputs are padded with a `[PAD]` or EOS token**
- **Long inputs are truncated to the maximum length**

Without padding, batch processing and parallel training would be inefficient since the model would need to perform individual calculations for each input.

### What is a Neural Network and What Are Weights? (Simplified Explanation)

- A **neural network** can be imagined as a vast network of connections processing data. Each **neuron** acts like a small switch deciding which information is passed on.
- **Weights** are numerical values determining the strength of a connection between neurons. When the model is trained, it adjusts these weights to make better decisions.
- Initially, the model makes many mistakes, but through repeated training, it learns which words typically follow others.

### How Does the Model Predict the Next Text?

The model analyzes a sequence of words and calculates which word is most likely to follow next. This process happens in several steps:
1. **Convert input text into tokens** ‚Äì The text is transformed into numerical values.
2. **A neural network processes the tokens** ‚Äì The sequence of tokens is processed through multiple network layers.
3. **Calculate probabilities for the next token** ‚Äì The model generates a list of possible next tokens with respective probabilities.
4. **Select the most likely token** ‚Äì The model chooses the most probable word and outputs it as the next token.
5. **Repeat until the maximum text length is reached** ‚Äì This process continues until the desired text length is generated.

## 3. What is Hugging Face and Why Do We Use It?

**Hugging Face** is a platform for open-source AI models. It provides pre-trained models, datasets, and libraries that simplify working with LLMs. In this project, we use:

- **Transformers**: A library for loading and training pre-trained models.
- **Datasets**: Enables loading and preparing text datasets.
- **Hugging Face Hub**: A platform to upload and share models with others.

This project uses GPT-2 from Hugging Face, but other models can also be used. Alternatives include:
- **GPT-2 Medium/Large/XL** ‚Äì Larger versions of GPT-2 for improved performance.
- **GPT-Neo (EleutherAI)** ‚Äì An open-source model with more advanced architectures.
- **BLOOM** ‚Äì A multilingual open-source LLM.

## 4. Features and Limitations

### Features

- **Text Generation**: The model generates text based on user input.
- **Customizability**: Parameters such as training duration, token length, or batch size can be adjusted.
- **Offline Usage**: No API access required, as the model runs locally.
- **Custom Training Data**: Any dataset can be used for fine-tuning.

### Limitations

- **No long-term memory**: Limited ability to remember previous interactions.
- **Limited training data**: The model is trained on only about 10,000 OpenWebText samples (adjustable parameter).
- **Simple architecture**: No advanced training methods like RLHF (Reinforcement Learning from Human Feedback).

## 5. Installation and Usage

### Clone the Project and Set Up the Environment

```bash
git clone https://github.com/your-username/MINILLM.git
cd MINILLM
python3 -m venv venv
```
Depending on your operating system, activate the virtual environment:
```bash
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate  # Windows
```

### Install Dependencies

The project requires some libraries that can be installed via `requirements.txt`:
```bash
pip install -r requirements.txt
```
This includes:
- **transformers**: Library from Hugging Face for working with pre-trained models.
- **torch**: PyTorch for training neural networks.
- **datasets**: A helper library for loading and processing datasets.
- **accelerate**: Optimizes training on different hardware setups.
- **huggingface_hub**: Interface for interacting with the Hugging Face platform.

### Train the Model (Takes 1‚Äì6 Hours Depending on Hardware)

```bash
python train_llm.py
```
In `train_llm.py`, the dataset can be changed by modifying the following line:
```python
dataset = load_dataset("openwebtext", trust_remote_code=True)
```
Alternatively, you can use your own training data, such as:
- **Wikipedia data** (`wikipedia` via Hugging Face Datasets)
- **Common Crawl** (Web text data)
- **Personal documents or specialized texts**

## 6. Adjustable Parameters

| Parameter                     | Description                                      | Default Value | Alternative Values |
|------------------------------|------------------------------------------------|--------------|----------------|
| `num_train_epochs`            | Number of training epochs                        | `2`          | `1 - 10` (depends on performance) |
| `max_length`                  | Maximum token length for inputs                 | `256`        | `128, 512, 1024` |
| `per_device_train_batch_size` | Number of samples per training step             | `2`          | `1 - 16` (depends on RAM) |
| `learning_rate`               | Learning rate of the model                      | `5e-5`       | `1e-4, 3e-5` |
| `tokenizer.pad_token`         | Default pad token for GPT-2                     | `tokenizer.eos_token` | `"[PAD]"` |

## 7. Key Terms Explained in Simple Words

- **Token**: The smallest unit of a text (word or character) that the model processes.
- **Tokenization**: The process of converting text into individual tokens before the model can understand it.
- **Batched** Processing: Training and inference are done in batches (groups of data) to optimize computational efficiency.
- **Truncation**: Cutting off long inputs to fit within the model‚Äôs maximum token limit.
- **Padding**: Filling shorter inputs with extra tokens to ensure all inputs have the same length.
- **Epochs**: The number of times the model goes through the entire dataset during training.
- **Pipeline**: A simplified Hugging Face interface for handling various NLP tasks.
- **Dataset**: A collection of texts or data used for training the model.

## 8. Interesting Facts About AI

- **10-15% of total computing power** in modern LLMs is used to reduce bias in generated text.
- **Modern AI models have more parameters than the human brain has synapses** ‚Äì GPT-4 is estimated to have over **one trillion** parameters.
- **Data quality is more important than data quantity** ‚Äì A small, clean dataset often yields better results than massive amounts of unstructured data.
- **AI requires constant fine-tuning** ‚Äì Even advanced models must be retrained regularly to stay up to date.

## 9. Further Development

- More training data for better results.
- Creating a web interface with Gradio or Streamlit.
- Uploading the model to Hugging Face.

Feel free to contribute, provide feedback, or collaborate!



German/Deutsch üá©üá™

# MINILLM ‚Äì Simpler Chatbot auf GPT-Basis

**MINILLM** ist ein kompaktes, selbst entwickeltes LLM (Large Language Model). Es zielt darauf ab, sowohl mir als auch anderen ein besseres Verst√§ndnis f√ºr KI-Modelle zu vermitteln. Es bietet eine einfache M√∂glichkeit, ein Sprachmodell zu trainieren, zu testen und anzupassen. Dieses Projekt richtet sich sowohl an Interessierte als auch an Entwickler, die selbst experimentieren m√∂chten. Dabei kann hier jeder das Projekt auf seinen Rechner kopieren.

## 1. √úberblick √ºber MINILLM

### Bekannte LLMs im Vergleich

| Modell       | Entwickler      | Besonderheit                                            |
| ------------ | --------------- | ------------------------------------------------------- |
| **ChatGPT**  | OpenAI          | Vielseitige Chat-KI, trainiert auf riesigen Datens√§tzen |
| **Grok**     | xAI (Elon Musk) | Humorvoller Chatbot mit zugespitzten Antworten          |
| **DeepSeek** | Open-Source     | Leistungseffizientes, freies LLM aus China              |

MINILLM ist eine **minimalistische Version dieser Modelle**, die lokal l√§uft und anpassbar ist.

## 2. Der gesamte Prozess ‚Äì Vom Modell zum Chatbot

Der Aufbau eines Sprachmodells wie MINILLM verl√§uft in mehreren Schritten:

1. **Ein vortrainiertes Modell laden** ‚Äì Wir nutzen ein bereits existierendes Modell (GPT-2) von Hugging Face, anstatt ein Modell von Grund auf zu trainieren. 
2. **Datens√§tze zum Trainieren bereitstellen** ‚Äì Das Modell wird mit einem Datensatz (z. B. OpenWebText) weitertrainiert, um es besser auf gew√ºnschte Eingaben anzupassen.
3. **Tokenization & Verarbeitung** ‚Äì Texte werden in Tokens umgewandelt, damit das Modell sie verstehen kann. K√ºrzere Texte werden mit Padding erg√§nzt, lange Texte gegebenenfalls abgeschnitten.
4. **Training des Modells** ‚Äì Das Modell durchl√§uft die Trainingsdaten mehrmals (Epochen) und lernt, Texte vorherzusagen, indem es das n√§chste Token basierend auf vorherigen Tokens berechnet. Dabei werden Gewichte in den neuronalen Netzwerken angepasst, sodass das Modell Muster erkennt und verbessert.
5. **Feinabstimmung und Tests** ‚Äì Nach dem Training wird das Modell getestet und kann f√ºr verschiedene Anwendungen genutzt werden.

### Was ist OpenWebText f√ºr ein Datensatz?

OpenWebText ist ein frei zug√§nglicher Datensatz, der aus qualitativ hochwertigen Webtexten besteht. Er wurde als Alternative zu OpenAI‚Äôs GPT-2-Trainingsdaten erstellt und enth√§lt Texte von vertrauensw√ºrdigen Quellen, die nach bestimmten Qualit√§tskriterien gefiltert wurden.

### Warum m√ºssen alle Eingaben die gleiche L√§nge haben? (Padding & Truncation)

Neuronale Netzwerke arbeiten effizienter, wenn alle Eingaben die gleiche L√§nge haben. Damit das Modell in festgelegten Bl√∂cken trainieren kann, werden:
Kurze Eingaben mit einem Padding-Token ([PAD] oder EOS-Token) aufgef√ºllt
Lange Eingaben auf die maximale L√§nge gek√ºrzt (Truncation)
Ohne Padding w√ºrden Batch-Verarbeitung und paralleles Training ineffizient laufen, weil das Modell sonst f√ºr jede Eingabe eine eigene Berechnung starten m√ºsste.

### Was ist ein neuronales Netzwerk und was sind Gewichte? (Einfach erkl√§rt)

Ein neuronales Netzwerk kann man sich wie ein riesiges Netz aus Verbindungen vorstellen, das Daten verarbeitet. Jedes Neuron ist wie ein kleiner Schalter, der entscheidet, welche Information weitergegeben wird.
Gewichte sind Zahlen, die bestimmen, wie stark eine Verbindung zwischen Neuronen ist. Wenn das Modell trainiert wird, passt es die Gewichte an, um bessere Entscheidungen zu treffen.
Am Anfang trifft das Modell viele Fehler, aber durch wiederholtes Training lernt es, welche W√∂rter typischerweise auf andere folgen.

### Wie genau sagt das Modell den n√§chsten Text vorher?

Das Modell betrachtet eine Sequenz von W√∂rtern und berechnet, welches Wort am wahrscheinlichsten als n√§chstes folgen sollte. Dies geschieht in mehreren Schritten:
Eingabetext in Tokens umwandeln ‚Äì Der Text wird in numerische Werte umgewandelt.
Ein neuronales Netzwerk verarbeitet die Tokens ‚Äì Die Token-Reihenfolge wird durch verschiedene Ebenen des Netzwerks verarbeitet.
Wahrscheinlichkeiten f√ºr das n√§chste Token berechnen ‚Äì Das Modell gibt eine Liste m√∂glicher n√§chster Tokens mit jeweiligen Wahrscheinlichkeiten aus.
Das wahrscheinlichste Token ausw√§hlen ‚Äì Das Modell entscheidet sich f√ºr das wahrscheinlichste Wort und gibt es als n√§chstes Token aus.
Wiederholung bis zur maximalen Textl√§nge ‚Äì Dieser Prozess wiederholt sich, bis die gew√ºnschte Textl√§nge erreicht ist.

## 3. Was ist Hugging Face und warum nutzen wir es?

**Hugging Face** ist eine Plattform f√ºr Open-Source-KI-Modelle. Es stellt vortrainierte Modelle, Datens√§tze und Bibliotheken zur Verf√ºgung, die die Arbeit mit LLMs erleichtern. In diesem Projekt nutzen wir:

- **Transformers**: Bibliothek f√ºr das Laden und Trainieren von vortrainierten Modellen.
- **Datasets**: Erm√∂glicht das Laden und Vorbereiten von Textdatens√§tzen.
- **Hugging Face Hub**: Eine Plattform, um Modelle hochzuladen und mit anderen zu teilen.

Das Modell wird mit GPT-2 von Hugging Face trainiert, aber es k√∂nnen auch andere Modelle genutzt werden. Alternativen sind:
- **GPT-2 Medium/Large/XL** ‚Äì Gr√∂√üere Versionen von GPT-2 f√ºr bessere Ergebnisse.
- **GPT-Neo (EleutherAI)** ‚Äì Ein Open-Source-Modell mit leistungsst√§rkeren Architekturen.
- **BLOOM** ‚Äì Ein mehrsprachiges Open-Source-LLM.

## 4. Funktionen und Einschr√§nkungen

### Funktionen

- **Texterzeugung**: Das Modell generiert Texte auf Basis von Benutzereingaben.
- **Anpassbarkeit**: Parameter wie Trainingsdauer, Token-L√§nge oder Batch-Gr√∂√üe sind individuell verstellbar.
- **Offline-Nutzung**: Kein API-Zugriff erforderlich, das Modell l√§uft lokal.
- **Eigene Trainingsdaten**: Beliebige Datens√§tze k√∂nnen f√ºr Feintuning genutzt werden.

### Einschr√§nkungen

- **Kein langfristiger Kontext**: Begrenzte F√§higkeit, sich an vorherige Aussagen zu erinnern.
- **Beschr√§nkte Trainingsdaten**: Das Modell lernt nur aus ca. 10.000 OpenWebText-Beispielen, der Parameter ist anpassbar.
- **Einfache Architektur**: Keine fortgeschrittenen Trainingsmethoden wie RLHF (Reinforcement Learning from Human Feedback).

## 5. Nutzung und Installation

### Projekt klonen und Umgebung vorbereiten

```bash
git clone https://github.com/dein-username/MINILLM.git
cd MINILLM
python3 -m venv venv
```
Je nach Betriebssystem muss die virtuelle Umgebung unterschiedlich aktiviert werden:
```bash
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate  # Windows
```

### Abh√§ngigkeiten installieren

Das Projekt ben√∂tigt einige Bibliotheken, die √ºber `requirements.txt` installiert werden:
```bash
pip install -r requirements.txt
```
Diese beinhalten unter anderem:
- **transformers**: Bibliothek von Hugging Face zur Arbeit mit vortrainierten Sprachmodellen.
- **torch**: PyTorch f√ºr das Training von neuronalen Netzen.
- **datasets**: Hilfsbibliothek zum Laden und Verarbeiten von Datens√§tzen.
- **accelerate**: Optimiert das Training auf verschiedenen Hardware-Setups.
- **huggingface_hub**: Schnittstelle zur Interaktion mit der Hugging Face Plattform.

### Modell trainieren (Dauer je nach Hardware zwischen 1‚Äì6 Stunden)

```bash
python train_llm.py
```
In der Datei `train_llm.py` kann der Datensatz ge√§ndert werden, indem die Quelle in folgender Zeile angepasst wird:
```python
dataset = load_dataset("openwebtext", trust_remote_code=True)
```
Alternativ k√∂nnen hier eigene Trainingsdaten eingebunden werden, z. B.:
- **Wikipedia-Daten** (`wikipedia` √ºber Hugging Face Datasets)
- **Common Crawl** (Web-Texte)
- **Pers√∂nliche Dokumente oder Spezialtexte**

## 6. Parameter zur Anpassung

| Parameter                     | Bedeutung                                         | Standardwert          | Alternative Werte |
| ----------------------------- | ------------------------------------------------- | --------------------- | ---------------- |
| `num_train_epochs`            | Anzahl der Trainingsdurchl√§ufe                    | `2`                   | `1 - 10` (je nach Leistung) |
| `max_length`                  | Maximale Tokenl√§nge f√ºr Eingaben                  | `256`                 | `128, 512, 1024` |
| `per_device_train_batch_size` | Anzahl der Samples pro Trainingsschritt           | `2`                   | `1 - 16` (abh√§ngig vom RAM) |
| `learning_rate`               | Lerngeschwindigkeit des Modells                   | `5e-5`                 | `1e-4, 3e-5` |
| `tokenizer.pad_token`         | Kein Standard-Pad-Token in GPT-2, daher EOS-Token | `tokenizer.eos_token` | `"[PAD]"` |

## 7. Fachbegriffe verst√§ndlich erkl√§rt

- **Token**: Eine kleinste Einheit eines Textes (Wort oder Zeichen), die das Modell verarbeitet.
- **Tokenization**: Der Prozess, Text in einzelne Tokens umzuwandeln, bevor das Modell sie verarbeitet.
- **Batched Processing**: Training und Inferenz werden in Stapeln (`Batches`) verarbeitet, um Rechenleistung effizient zu nutzen.
- **Truncation**: K√ºrzung langer Eingaben, um die maximale Tokenanzahl einzuhalten.
- **Padding**: Auff√ºllen von zu kurzen Eingaben, um gleiche L√§ngen zu gew√§hrleisten.
- **Epochen**: Anzahl der Trainingsdurchl√§ufe durch den gesamten Datensatz.
- **Pipeline**: Eine vereinfachte Schnittstelle von Hugging Face f√ºr verschiedene NLP-Aufgaben.
- **Dataset**: Eine Sammlung von Texten oder Daten, die f√ºr das Training genutzt werden.

## 8. Interessante Fakten √ºber K√ºnstliche Intelligenz (KI)

10-15 % der gesamten Rechenleistung in modernen LLMs wird verwendet, um Bias (Voreingenommenheit) in den generierten Texten zu minimieren.
Moderne KI-Modelle haben mehr Parameter als das menschliche Gehirn Synapsen besitzt ‚Äì GPT-4 soll √ºber eine Billion Parameter haben.
Datenqualit√§t ist wichtiger als Datenmenge ‚Äì Ein kleiner, sauberer Datensatz kann oft bessere Ergebnisse liefern als riesige Mengen unstrukturierter Daten.
KI ben√∂tigt st√§ndiges Feintuning ‚Äì Selbst fortgeschrittene Modelle m√ºssen regelm√§√üig nachtrainiert werden, um mit neuen Informationen Schritt zu halten.

## 9. Weitere Entwicklungsm√∂glichkeiten

- Mehr Trainingsdaten f√ºr bessere Ergebnisse.
- Erstellung eines Web-Interfaces mit Gradio oder Streamlit.
- Hochladen des Modells auf Hugging Face.

Ich freue mich √ºber Beitr√§ge, Kritik und Kommentare.


